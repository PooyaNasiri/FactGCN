{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RgThOaKLbdrQ",
    "outputId": "e8dd1982-7b3b-41f7-b69e-4b38932dbefe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Looking in links: https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
      "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.11/dist-packages (2.1.2+pt20cpu)\n",
      "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.11/dist-packages (0.6.18+pt20cpu)\n",
      "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.11/dist-packages (1.6.3+pt20cpu)\n",
      "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.11/dist-packages (1.2.2+pt20cpu)\n",
      "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.11/dist-packages (2.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-sparse) (1.13.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.11)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2024.12.14)\n",
      "PyTorch Geometric version: 2.6.1\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch\n",
    "!pip install torch torchvision torchaudio\n",
    "\n",
    "# Install PyTorch Geometric and dependencies\n",
    "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-2.0.0+cpu.html\n",
    "\n",
    "# Verify Installation\n",
    "# Import necessary libraries\n",
    "import torch_geometric\n",
    "print(\"PyTorch Geometric version:\", torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T15:09:59.632264Z",
     "iopub.status.busy": "2024-12-15T15:09:59.631794Z",
     "iopub.status.idle": "2024-12-15T15:09:59.638275Z",
     "shell.execute_reply": "2024-12-15T15:09:59.63691Z",
     "shell.execute_reply.started": "2024-12-15T15:09:59.632225Z"
    },
    "id": "tJA3lpcecyf5"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "# Import necessary libraries\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "# Import necessary libraries\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from scipy.sparse import coo_matrix\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.utils import to_scipy_sparse_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T15:09:59.941909Z",
     "iopub.status.busy": "2024-12-15T15:09:59.941461Z",
     "iopub.status.idle": "2024-12-15T15:09:59.949125Z",
     "shell.execute_reply": "2024-12-15T15:09:59.947916Z",
     "shell.execute_reply.started": "2024-12-15T15:09:59.94187Z"
    },
    "id": "r4Ir64Focyf6"
   },
   "outputs": [],
   "source": [
    "\n",
    "#First version of FastGCNLayer\n",
    "class FastGCNLayer(tf.keras.layers.Layer):\n",
    "# Function definition\n",
    "    def __init__(self, output_dim, activation=None, dropout_rate=0.0, **kwargs):\n",
    "        super(FastGCNLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "# Function definition\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.output_dim),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(FastGCNLayer, self).build(input_shape)\n",
    "\n",
    "# Function definition\n",
    "    def call(self, features, adj, sampled_nodes, training=False):\n",
    "        # Convert sampled_nodes to int64 to match adj.indices data type\n",
    "        sampled_nodes = tf.cast(sampled_nodes, dtype=tf.int64)\n",
    "\n",
    "# Loop execution\n",
    "        # Filter adjacency matrix rows and columns for sampled nodes\n",
    "        row_mask = tf.reduce_any(adj.indices[:, 0:1] == sampled_nodes, axis=1)\n",
    "        col_mask = tf.reduce_any(adj.indices[:, 1:2] == sampled_nodes, axis=1)\n",
    "        sampled_adj_indices = tf.boolean_mask(adj.indices, tf.logical_and(row_mask, col_mask))\n",
    "        sampled_adj_values = tf.boolean_mask(adj.values, tf.logical_and(row_mask, col_mask))\n",
    "\n",
    "        # Remap node indices to the range 0 to len(sampled_nodes) - 1\n",
    "        unique_nodes, remapped_indices = tf.unique(tf.reshape(sampled_adj_indices, [-1]))\n",
    "\n",
    "        # Explicitly cast tf.range to int64 to match expected data type\n",
    "        node_map = tf.lookup.StaticHashTable(\n",
    "            tf.lookup.KeyValueTensorInitializer(unique_nodes, tf.range(tf.size(unique_nodes), dtype=tf.int64)), # Cast to int64\n",
    "            -1\n",
    "        )\n",
    "        remapped_indices = tf.reshape(node_map.lookup(tf.reshape(sampled_adj_indices, [-1])), tf.shape(sampled_adj_indices))\n",
    "\n",
    "        # Get the actual number of valid sampled nodes\n",
    "        num_valid_nodes = tf.shape(unique_nodes)[0]\n",
    "\n",
    "        sampled_adj = tf.sparse.SparseTensor(\n",
    "            indices=remapped_indices, #Use remapped indices here\n",
    "            values=sampled_adj_values,\n",
    "            # Use num_valid_nodes instead of len(sampled_nodes)\n",
    "            dense_shape=[num_valid_nodes, num_valid_nodes]\n",
    "        )\n",
    "        sampled_adj = tf.sparse.reorder(sampled_adj)\n",
    "\n",
    "# Loop execution\n",
    "        # Extract features for sampled nodes\n",
    "        # Remap sampled_nodes to the new range (0 to num_samples - 1)\n",
    "# Loop execution\n",
    "        # This is crucial for the second layer\n",
    "        sampled_nodes_remapped = node_map.lookup(sampled_nodes)\n",
    "\n",
    "        # Filter out invalid indices (-1)\n",
    "        valid_indices_mask = tf.where(sampled_nodes_remapped >= 0)\n",
    "        sampled_nodes_remapped = tf.gather_nd(sampled_nodes_remapped, valid_indices_mask)\n",
    "\n",
    "# Loop execution\n",
    "        # Gather features for valid indices only\n",
    "        sampled_features = tf.gather(features, sampled_nodes_remapped, validate_indices=False)\n",
    "\n",
    "        # Perform sparse neighborhood aggregation\n",
    "        aggregated_features = tf.sparse.sparse_dense_matmul(sampled_adj, sampled_features)\n",
    "        aggregated_features = tf.matmul(aggregated_features, self.kernel)\n",
    "\n",
    "        # Apply activation and dropout\n",
    "# Conditional check\n",
    "        if self.activation:\n",
    "            aggregated_features = self.activation(aggregated_features)\n",
    "        return self.dropout(aggregated_features, training=training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "MwTsU4j6nAYY",
    "outputId": "8701ba32-7493-45ed-9584-adbc2e1a6a0c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nFirst Version of train function ( the one originally used for CORA )\\n\\n\\n\\n# Training Function\\ndef train(model, features, adj, labels, train_mask, num_epochs, num_samples, learning_rate):\\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\\n    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\\n\\n    for epoch in range(num_epochs):\\n        with tf.GradientTape() as tape:\\n            # Importance sampling\\n            sampled_nodes = importance_sampling(adj, num_samples)\\n\\n            # Forward pass\\n            logits = model(features, adj, sampled_nodes, training=True)\\n\\n            # Ensure valid_sampled_nodes are less than the size of logits\\n            valid_sampled_indices = tf.where(sampled_nodes < tf.shape(logits)[0])[:, 0]\\n\\n            # Clip valid_sampled_indices to be within the range of logits\\n            # Cast tf.shape(logits)[0] - 1 to int64 to match valid_sampled_indices\\n            valid_sampled_indices = tf.clip_by_value(valid_sampled_indices, 0, tf.cast(tf.shape(logits)[0] - 1, tf.int64)) # Changed to int64\\n\\n            # Gather labels and logits for valid sampled nodes only\\n            sampled_labels = tf.gather(labels, tf.gather(sampled_nodes, valid_sampled_indices))\\n            sampled_logits = tf.gather(logits, valid_sampled_indices)\\n\\n            # Compute loss\\n            loss = loss_fn(sampled_labels, sampled_logits)\\n\\n            # Backward pass\\n            gradients = tape.gradient(loss, model.trainable_variables)\\n            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\\n\\n           # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.numpy():.4f}\")\\n            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.numpy().item():.4f}\")\\n'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Loop execution\n",
    "First Version of train function ( the one originally used for CORA )\n",
    "\n",
    "\n",
    "\n",
    "# Training Function\n",
    "# Function definition\n",
    "def train(model, features, adj, labels, train_mask, num_epochs, num_samples, learning_rate):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Loop execution\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Importance sampling\n",
    "            sampled_nodes = importance_sampling(adj, num_samples)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(features, adj, sampled_nodes, training=True)\n",
    "\n",
    "            # Ensure valid_sampled_nodes are less than the size of logits\n",
    "            valid_sampled_indices = tf.where(sampled_nodes < tf.shape(logits)[0])[:, 0]\n",
    "\n",
    "            # Clip valid_sampled_indices to be within the range of logits\n",
    "            # Cast tf.shape(logits)[0] - 1 to int64 to match valid_sampled_indices\n",
    "            valid_sampled_indices = tf.clip_by_value(valid_sampled_indices, 0, tf.cast(tf.shape(logits)[0] - 1, tf.int64)) # Changed to int64\n",
    "\n",
    "# Loop execution\n",
    "            # Gather labels and logits for valid sampled nodes only\n",
    "            sampled_labels = tf.gather(labels, tf.gather(sampled_nodes, valid_sampled_indices))\n",
    "            sampled_logits = tf.gather(logits, valid_sampled_indices)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(sampled_labels, sampled_logits)\n",
    "\n",
    "            # Backward pass\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "           # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.numpy():.4f}\")\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.numpy().item():.4f}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "id": "C0dB2ygDl6LL",
    "outputId": "bfe638f6-157b-4075-fce9-7e80a17b5d10"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nclass FastGCNLayer(tf.keras.layers.Layer):\\n    def __init__(self, output_dim, activation=None, dropout_rate=0.0, **kwargs):\\n        super(FastGCNLayer, self).__init__(**kwargs)\\n        self.output_dim = output_dim\\n        self.activation = activation\\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\\n\\n    def build(self, input_shape):\\n        self.kernel = self.add_weight(\\n            shape=(input_shape[-1], self.output_dim),\\n            initializer=\"glorot_uniform\",\\n            trainable=True,\\n        )\\n        super(FastGCNLayer, self).build(input_shape)\\n\\n    def call(self, features, adj, sampled_nodes, training=False):\\n        # Convert sampled_nodes to int64\\n        sampled_nodes = tf.cast(sampled_nodes, dtype=tf.int64)\\n\\n        # First transform the features using the kernel\\n        transformed_features = tf.matmul(features, self.kernel)\\n\\n        # Create a mask for the sampled nodes but using the shape of transformed_features\\n        batch_size = tf.shape(transformed_features)[0] #changed here\\n        num_nodes = tf.shape(transformed_features)[1] #changed here\\n\\n        # Create a sparse mask matrix for sampling using new batch_size and num_nodes values\\n        indices = tf.stack([\\n            tf.range(batch_size, dtype=tf.int64),\\n            tf.cast(sampled_nodes, tf.int64)\\n        ], axis=1)\\n\\n        values = tf.ones([batch_size], dtype=tf.float32)\\n        mask = tf.sparse.SparseTensor(\\n            indices=indices,\\n            values=values,\\n            dense_shape=[batch_size, num_nodes] #using new values here\\n        )\\n\\n        # Sample the adjacency matrix using the mask\\n        sampled_features = tf.sparse.sparse_dense_matmul(mask, transformed_features)\\n\\n        # Apply activation and dropout\\n        if self.activation:\\n            sampled_features = self.activation(sampled_features)\\n\\n        return self.dropout(sampled_features, training=training)\\n'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "  Second version of FastGCNLayer with this changes:\n",
    "\n",
    "# Loop execution\n",
    "  Added layer normalization for better training stability\n",
    "  Improved adjacency matrix normalization\n",
    "  Added bias term option\n",
    "  More efficient sparse matrix operations\n",
    "\n",
    "  DON'T USE DOES NOT WORKING\n",
    "'''\n",
    "'''\n",
    "class FastGCNLayer(tf.keras.layers.Layer):\n",
    "# Function definition\n",
    "    def __init__(self, output_dim, activation=None, dropout_rate=0.0, **kwargs):\n",
    "        super(FastGCNLayer, self).__init__(**kwargs)\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "# Function definition\n",
    "    def build(self, input_shape):\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_shape[-1], self.output_dim),\n",
    "            initializer=\"glorot_uniform\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        super(FastGCNLayer, self).build(input_shape)\n",
    "\n",
    "# Function definition\n",
    "    def call(self, features, adj, sampled_nodes, training=False):\n",
    "        # Convert sampled_nodes to int64\n",
    "        sampled_nodes = tf.cast(sampled_nodes, dtype=tf.int64)\n",
    "\n",
    "        # First transform the features using the kernel\n",
    "        transformed_features = tf.matmul(features, self.kernel)\n",
    "\n",
    "# Loop execution\n",
    "        # Create a mask for the sampled nodes but using the shape of transformed_features\n",
    "        batch_size = tf.shape(transformed_features)[0] #changed here\n",
    "        num_nodes = tf.shape(transformed_features)[1] #changed here\n",
    "\n",
    "# Loop execution\n",
    "        # Create a sparse mask matrix for sampling using new batch_size and num_nodes values\n",
    "        indices = tf.stack([\n",
    "            tf.range(batch_size, dtype=tf.int64),\n",
    "            tf.cast(sampled_nodes, tf.int64)\n",
    "        ], axis=1)\n",
    "\n",
    "        values = tf.ones([batch_size], dtype=tf.float32)\n",
    "        mask = tf.sparse.SparseTensor(\n",
    "            indices=indices,\n",
    "            values=values,\n",
    "            dense_shape=[batch_size, num_nodes] #using new values here\n",
    "        )\n",
    "\n",
    "        # Sample the adjacency matrix using the mask\n",
    "        sampled_features = tf.sparse.sparse_dense_matmul(mask, transformed_features)\n",
    "\n",
    "        # Apply activation and dropout\n",
    "# Conditional check\n",
    "        if self.activation:\n",
    "            sampled_features = self.activation(sampled_features)\n",
    "\n",
    "        return self.dropout(sampled_features, training=training)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ITREtx1EnnR1"
   },
   "outputs": [],
   "source": [
    "# FastGCN Model\n",
    "class FastGCN(tf.keras.Model):\n",
    "# Function definition\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.5):\n",
    "        super(FastGCN, self).__init__()\n",
    "        self.layer1 = FastGCNLayer(hidden_dim, activation=tf.nn.relu, dropout_rate=dropout_rate)\n",
    "        self.layer2 = FastGCNLayer(output_dim, activation=None, dropout_rate=dropout_rate)\n",
    "\n",
    "# Function definition\n",
    "    def call(self, features, adj, sampled_nodes, training=False):\n",
    "        x = self.layer1(features, adj, sampled_nodes, training=training)\n",
    "        x = self.layer2(x, adj, sampled_nodes, training=training)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T15:10:00.252852Z",
     "iopub.status.busy": "2024-12-15T15:10:00.252428Z",
     "iopub.status.idle": "2024-12-15T15:10:00.261134Z",
     "shell.execute_reply": "2024-12-15T15:10:00.259671Z",
     "shell.execute_reply.started": "2024-12-15T15:10:00.252794Z"
    },
    "id": "Zv9dJKl0cyf7"
   },
   "outputs": [],
   "source": [
    "# Importance Sampling Function\n",
    "# Function definition\n",
    "def importance_sampling(adj, num_samples):\n",
    "    degrees = tf.sparse.reduce_sum(adj, axis=1) + 1e-5  # Avoid division by zero\n",
    "    probabilities = degrees / tf.reduce_sum(degrees)\n",
    "    sampled_nodes = np.random.choice(adj.shape[0], num_samples, replace=False, p=probabilities.numpy())\n",
    "    return tf.constant(sampled_nodes, dtype=tf.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AYN1prRFpUe"
   },
   "outputs": [],
   "source": [
    "\n",
    "#train used with mnist dataset and PubMed\n",
    "# Function definition\n",
    "def train(model, features, adj, labels, train_mask, num_epochs, num_samples, learning_rate):\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Ensure num_samples is not larger than the number of nodes\n",
    "    num_samples = min(num_samples, features.shape[0])\n",
    "\n",
    "# Loop execution\n",
    "    for epoch in range(num_epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Importance sampling\n",
    "            sampled_nodes = importance_sampling(adj, num_samples)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(features, adj, sampled_nodes, training=True)\n",
    "\n",
    "            # Ensure we only use valid indices\n",
    "            valid_mask = sampled_nodes < tf.shape(logits)[0]\n",
    "            valid_sampled_nodes = tf.boolean_mask(sampled_nodes, valid_mask)\n",
    "\n",
    "# Loop execution\n",
    "            # Gather labels and logits for valid sampled nodes\n",
    "            sampled_labels = tf.gather(labels, valid_sampled_nodes)\n",
    "            sampled_logits = tf.gather(logits, tf.range(tf.shape(valid_sampled_nodes)[0]))\n",
    "\n",
    "            # Compute loss and reduce to scalar\n",
    "            loss = tf.reduce_mean(loss_fn(sampled_labels, sampled_logits))\n",
    "\n",
    "# Conditional check\n",
    "        # Check if loss is valid before proceeding\n",
    "# Conditional check\n",
    "        if tf.math.is_nan(loss):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Warning: NaN loss detected\")\n",
    "            continue\n",
    "\n",
    "        # Compute and clip gradients\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "\n",
    "# Loop execution\n",
    "        # Check for valid gradients\n",
    "# Loop execution\n",
    "        if any(tf.reduce_any(tf.math.is_nan(g)) if g is not None else False for g in gradients):\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Warning: NaN gradients detected\")\n",
    "            continue\n",
    "\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        # Convert loss to Python scalar before printing\n",
    "        try:\n",
    "            loss_value = float(loss.numpy())\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss_value:.4f}\")\n",
    "        except:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Error printing loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALWpQM6UmsL4"
   },
   "outputs": [],
   "source": [
    "#batched version\n",
    "# Function definition\n",
    "def create_graph_structure(features, k=15, metric='euclidean', batch_size=1000):\n",
    "\n",
    "# Loop execution\n",
    "    \"\"\"Create k-nearest neighbors graph for MNIST with batched processing\"\"\"\n",
    "    print(\"Creating MNIST graph structure...\")\n",
    "    num_samples = len(features)\n",
    "    edges = []\n",
    "\n",
    "# Loop execution\n",
    "    for i in range(0, num_samples, batch_size):\n",
    "        end = min(i + batch_size, num_samples)\n",
    "        batch_features = features[i:end]\n",
    "\n",
    "# Loop execution\n",
    "        # Compute distances for current batch\n",
    "        distances = cdist(batch_features, features, metric=metric)\n",
    "\n",
    "        # Find k nearest neighbors\n",
    "        nearest = np.argpartition(distances, k+1, axis=1)[:, :k+1]\n",
    "\n",
    "# Loop execution\n",
    "        # Create edges (both directions for undirected graph)\n",
    "# Loop execution\n",
    "        for idx, neighbors in enumerate(nearest):\n",
    "            global_idx = i + idx\n",
    "            valid_neighbors = neighbors[neighbors != global_idx][:k]\n",
    "# Loop execution\n",
    "            edges.extend([(global_idx, n) for n in valid_neighbors])\n",
    "# Loop execution\n",
    "            edges.extend([(n, global_idx) for n in valid_neighbors])\n",
    "\n",
    "    # Create sparse adjacency matrix\n",
    "    edges = np.array(edges)\n",
    "    values = np.ones(len(edges), dtype=np.float32)\n",
    "    adj_matrix = coo_matrix(\n",
    "        (values, (edges[:, 0], edges[:, 1])),\n",
    "        shape=(num_samples, num_samples)\n",
    "    )\n",
    "\n",
    "    return tf.sparse.reorder(tf.sparse.SparseTensor(\n",
    "        indices=np.vstack((adj_matrix.row, adj_matrix.col)).T,\n",
    "        values=adj_matrix.data,\n",
    "        dense_shape=adj_matrix.shape\n",
    "    ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "levMHffCYFIZ"
   },
   "outputs": [],
   "source": [
    "# Function definition\n",
    "def load_mnist_file(filepath):\n",
    "\n",
    "    try:\n",
    "        data = pd.read_csv(filepath)\n",
    "\n",
    "        '''\n",
    "        labels = data['label'].values\n",
    "        features = data.drop('label', axis=1).values.astype(np.float32)\n",
    "        '''\n",
    "\n",
    "        labels = data.iloc[:, 0].values  # Estrai la prima colonna come etichette\n",
    "        features = data.iloc[:, 1:].values.astype(np.float32)  # Resto delle colonne sono le features\n",
    "\n",
    "        features = features / 255.0  # Normalize pixel values to [0, 1]\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kw22bMr1Ggc6"
   },
   "outputs": [],
   "source": [
    "# Loop execution\n",
    "# Function to check tensor properties (useful for debugging)\n",
    "# Function definition\n",
    "def print_tensor_info(tensor, name):\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Shape: {tensor.shape}\")\n",
    "    print(f\"  Dtype: {tensor.dtype}\")\n",
    "    print(f\"  Min: {tf.reduce_min(tensor)}\")\n",
    "    print(f\"  Max: {tf.reduce_max(tensor)}\")\n",
    "\n",
    "# Modified evaluation helper function\n",
    "# Function definition\n",
    "def evaluate_model(model, features, adj, labels, num_samples):\n",
    "    sampled_nodes = importance_sampling(adj, num_samples)\n",
    "    logits = model(features, adj, sampled_nodes, training=False)\n",
    "\n",
    "    valid_mask = sampled_nodes < tf.shape(logits)[0]\n",
    "    valid_sampled_nodes = tf.cast(tf.boolean_mask(sampled_nodes, valid_mask), tf.int64)\n",
    "\n",
    "    predictions = tf.cast(\n",
    "        tf.argmax(tf.gather(logits, tf.range(tf.shape(valid_sampled_nodes)[0])), axis=1),\n",
    "        tf.int64\n",
    "    )\n",
    "    labels_sampled = tf.gather(labels, valid_sampled_nodes)\n",
    "\n",
    "    accuracy = tf.reduce_mean(\n",
    "        tf.cast(tf.equal(predictions, labels_sampled), tf.float32)\n",
    "    )\n",
    "\n",
    "    return accuracy, predictions, labels_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7bpPDEykL5q",
    "outputId": "cf71b213-e2e1-4329-b370-2279e51492ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainData loaded successfully!\n",
      "TainFeatures shape: (19999, 784)\n",
      "trainLabels shape: (19999,)\n",
      "testData loaded successfully!\n",
      "TestFeatures shape: (9999, 784)\n",
      "TestLabels shape: (9999,)\n"
     ]
    }
   ],
   "source": [
    " # Load data\n",
    "\n",
    "train_features, train_labels = load_mnist_file(\"/content/sample_data/mnist_train_small.csv\")\n",
    "\n",
    "# Conditional check\n",
    "if train_features is not None and train_labels is not None:\n",
    "  print(\"trainData loaded successfully!\")\n",
    "  print(\"TainFeatures shape:\", train_features.shape)\n",
    "  print(\"trainLabels shape:\", train_labels.shape)\n",
    "\n",
    "test_features, test_labels = load_mnist_file(\"/content/sample_data/mnist_test.csv\")\n",
    "\n",
    "# Conditional check\n",
    "if test_features is not None and test_labels is not None:\n",
    "  print(\"testData loaded successfully!\")\n",
    "  print(\"TestFeatures shape:\", test_features.shape)\n",
    "  print(\"TestLabels shape:\", test_labels.shape)\n",
    "\n",
    "# Convert to TensorFlow tensors with explicit types\n",
    "train_features = tf.convert_to_tensor(train_features, dtype=tf.float32)\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int64)  # Changed to int64\n",
    "test_features = tf.convert_to_tensor(test_features, dtype=tf.float32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.int64)    # Changed to int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFn4h8tMkM1G",
    "outputId": "cb38ad78-3c1b-417d-f549-1bfa6beb3d0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training graph structure...\n",
      "Creating MNIST graph structure...\n",
      "\n",
      "Training Graph Debug Info:\n",
      "Number of nodes: 19999\n",
      "Number of edges: 599970 (counted as directed edges)\n",
      "Adjacency matrix density: 0.001500\n",
      "\n",
      "Creating testing graph structure...\n",
      "Creating MNIST graph structure...\n",
      "\n",
      "Testing Graph Debug Info:\n",
      "Number of nodes: 9999\n",
      "Number of edges: 299970 (counted as directed edges)\n",
      "Adjacency matrix density: 0.003000\n"
     ]
    }
   ],
   "source": [
    "# Create graph structure using k-nearest neighbors\n",
    "print(\"Creating training graph structure...\")\n",
    "train_adj = create_graph_structure(train_features.numpy())\n",
    "\n",
    "# Loop execution\n",
    "# Debugging information for training graph\n",
    "print(\"\\nTraining Graph Debug Info:\")\n",
    "print(f\"Number of nodes: {train_adj.shape[0]}\")\n",
    "print(f\"Number of edges: {train_adj.indices.shape[0]} (counted as directed edges)\")\n",
    "print(f\"Adjacency matrix density: {train_adj.indices.shape[0] / (train_adj.shape[0] ** 2):.6f}\")\n",
    "\n",
    "print(\"\\nCreating testing graph structure...\")\n",
    "test_adj = create_graph_structure(test_features.numpy())\n",
    "\n",
    "# Loop execution\n",
    "# Debugging information for testing graph\n",
    "print(\"\\nTesting Graph Debug Info:\")\n",
    "print(f\"Number of nodes: {test_adj.shape[0]}\")\n",
    "print(f\"Number of edges: {test_adj.indices.shape[0]} (counted as directed edges)\")\n",
    "print(f\"Adjacency matrix density: {test_adj.indices.shape[0] / (test_adj.shape[0] ** 2):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "XQExO4VTkW5k",
    "outputId": "07b70755-2b95-44d7-9103-f1a0ac490e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training with 500 samples per epoch...\n",
      "Epoch 1/50, Loss: 4.3535\n",
      "Epoch 2/50, Loss: 4.4343\n",
      "Epoch 3/50, Loss: 2.1507\n",
      "Epoch 4/50, Loss: 2.7139\n",
      "Epoch 5/50, Loss: 3.7070\n",
      "Epoch 6/50, Loss: 1.9476\n",
      "Epoch 7/50, Loss: 2.6127\n",
      "Epoch 8/50, Loss: 2.2940\n",
      "Epoch 9/50, Loss: 3.0446\n",
      "Epoch 10/50, Loss: 3.1528\n",
      "Epoch 11/50, Loss: 2.7352\n",
      "Epoch 12/50, Loss: 2.4396\n",
      "Epoch 13/50, Loss: 2.4598\n",
      "Epoch 14/50, Loss: 3.3334\n",
      "Epoch 15/50, Loss: 2.3016\n",
      "Epoch 16/50, Loss: 2.4532\n",
      "Epoch 17/50, Loss: 4.0046\n",
      "Epoch 18/50, Loss: 2.3977\n",
      "Epoch 19/50, Loss: 2.3872\n",
      "Epoch 20/50, Loss: 2.3758\n",
      "Epoch 21/50, Loss: 2.3297\n",
      "Epoch 22/50, Loss: 3.5467\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-fb32448af806>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m train(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-7b9a8abdd533>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, features, adj, labels, train_mask, num_epochs, num_samples, learning_rate)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;31m# Ensure we only use valid indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;31m# Change the layout for the layer output if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# This is useful for relayout intermediate tensor in the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             )\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Plain flow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-149-21e95e424178>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, features, adj, sampled_nodes, training)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampled_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    899\u001b[0m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;31m# Change the layout for the layer output if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# This is useful for relayout intermediate tensor in the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/ops/operation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mobject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.__class__.__name__}.call()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             )\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# Plain flow.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mbound_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_keras_call_info_injected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-146-f816c4e87568>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, features, adj, sampled_nodes, training)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Filter adjacency matrix rows and columns for sampled nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mrow_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msampled_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mcol_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msampled_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0msampled_adj_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboolean_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1260\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1261\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_any\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   3236\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   3237\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3238\u001b[0;31m       gen_math_ops._any(\n\u001b[0m\u001b[1;32m   3239\u001b[0m           \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ReductionDims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3240\u001b[0m           name=name))\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_any\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m    700\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    703\u001b[0m         _ctx, \"Any\", name, input, axis, \"keep_dims\", keep_dims)\n\u001b[1;32m    704\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "input_dim = train_features.shape[1]\n",
    "hidden_dim = 16\n",
    "output_dim = 10\n",
    "\n",
    "# Initialize model\n",
    "model = FastGCN(input_dim, hidden_dim, output_dim, dropout_rate=0.3)\n",
    "\n",
    "# Training parameters\n",
    "num_samples = min(500, train_features.shape[0])\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0005\n",
    "\n",
    "print(f\"Starting training with {num_samples} samples per epoch...\")\n",
    "\n",
    "# Train the model\n",
    "train(\n",
    "    model=model,\n",
    "    features=train_features,\n",
    "    adj=train_adj,\n",
    "    labels=train_labels,\n",
    "    train_mask=np.ones(len(train_features), dtype=bool),\n",
    "    num_epochs=num_epochs,\n",
    "    num_samples=num_samples,\n",
    "    learning_rate=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9OEWdV9ki1j"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluation\n",
    "print(\"\\nEvaluating model...\")\n",
    "try:\n",
    "  sampled_nodes_test = importance_sampling(test_adj, num_samples)\n",
    "  test_logits = model(test_features, test_adj, sampled_nodes_test, training=False)\n",
    "\n",
    "  # Ensure predictions and labels have the same shape and type\n",
    "  valid_mask = sampled_nodes_test < tf.shape(test_logits)[0]\n",
    "  valid_sampled_nodes = tf.boolean_mask(sampled_nodes_test, valid_mask)\n",
    "\n",
    "  # Make sure all tensors are the same type (int64)\n",
    "  valid_sampled_nodes = tf.cast(valid_sampled_nodes, tf.int64)\n",
    "  test_predictions = tf.cast(\n",
    "      tf.argmax(tf.gather(test_logits, tf.range(tf.shape(valid_sampled_nodes)[0])), axis=1),\n",
    "      tf.int64\n",
    "      )\n",
    "  test_labels_sampled = tf.gather(test_labels, valid_sampled_nodes)\n",
    "\n",
    "  # Calculate accuracy\n",
    "  test_accuracy = tf.reduce_mean(\n",
    "      tf.cast(tf.equal(test_predictions, test_labels_sampled), tf.float32)\n",
    "      )\n",
    "\n",
    "  print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "  print(f\"Error during evaluation: {str(e)}\")\n",
    "  print(\"Debug information:\")\n",
    "  print(f\"Test logits shape: {test_logits.shape}\")\n",
    "  print(f\"Valid sampled nodes shape: {valid_sampled_nodes.shape}\")\n",
    "  print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "  print(f\"Test labels sampled shape: {test_labels_sampled.shape}\")\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agRQK6F4bHjz"
   },
   "outputs": [],
   "source": [
    "# Load the PubMed dataset\n",
    "dataset = Planetoid(root='/tmp/PubMed', name='PubMed')\n",
    "data = dataset[0]  # Get the data object\n",
    "\n",
    "# Loop execution\n",
    "# Convert features and labels to NumPy for TensorFlow compatibility\n",
    "features = data.x.numpy().astype(np.float32)  # Feature matrix\n",
    "labels = data.y.numpy().astype(np.int64)  # Node labels\n",
    "\n",
    "# Convert adjacency matrix to SciPy sparse matrix\n",
    "adj_matrix = to_scipy_sparse_matrix(data.edge_index).tocoo()\n",
    "\n",
    "# Convert adjacency matrix to TensorFlow SparseTensor\n",
    "adj_tensor = tf.sparse.SparseTensor(\n",
    "    indices=np.vstack((adj_matrix.row, adj_matrix.col)).T,\n",
    "    values=adj_matrix.data,\n",
    "    dense_shape=adj_matrix.shape\n",
    ")\n",
    "adj_tensor = tf.sparse.reorder(adj_tensor)\n",
    "\n",
    "# Prepare train, validation, and test masks\n",
    "num_nodes = features.shape[0]\n",
    "\n",
    "train_mask = np.zeros(num_nodes, dtype=bool)\n",
    "train_mask[data.train_mask.numpy()] = True\n",
    "\n",
    "val_mask = np.zeros(num_nodes, dtype=bool)\n",
    "val_mask[data.val_mask.numpy()] = True\n",
    "\n",
    "test_mask = np.zeros(num_nodes, dtype=bool)\n",
    "test_mask[data.test_mask.numpy()] = True\n",
    "\n",
    "# Debugging information\n",
    "print(\"Dataset:\", dataset)\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Feature matrix shape:\", features.shape)\n",
    "print(\"Number of edges:\", adj_matrix.nnz)\n",
    "print(\"Number of classes:\", np.max(labels) + 1)\n",
    "print(\"Train nodes:\", np.sum(train_mask))\n",
    "print(\"Validation nodes:\", np.sum(val_mask))\n",
    "print(\"Test nodes:\", np.sum(test_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45A70-LGbsqi"
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_dim = features.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = labels.max() + 1\n",
    "\n",
    "# Initialize the FastGCN model\n",
    "model = FastGCN(input_dim, hidden_dim, output_dim, dropout_rate=0.5)\n",
    "\n",
    "# Training parameters\n",
    "num_samples = 500  # Number of nodes sampled per epoch\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Train the model\n",
    "train(\n",
    "    model=model,\n",
    "    features=features,\n",
    "    adj=adj_tensor,\n",
    "    labels=labels,\n",
    "    train_mask=train_mask,\n",
    "    num_epochs=num_epochs,\n",
    "    num_samples=num_samples,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy, _, _ = evaluate_model(\n",
    "    model=model,\n",
    "    features=features,\n",
    "    adj=adj_tensor,\n",
    "    labels=labels,\n",
    "    num_samples=np.sum(test_mask)\n",
    ")\n",
    "print(f\"Test Accuracy on PubMed: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T15:10:30.218257Z",
     "iopub.status.busy": "2024-12-15T15:10:30.217816Z",
     "iopub.status.idle": "2024-12-15T15:10:30.405328Z",
     "shell.execute_reply": "2024-12-15T15:10:30.403852Z",
     "shell.execute_reply.started": "2024-12-15T15:10:30.218222Z"
    },
    "id": "IfAYfQVFcyf7"
   },
   "outputs": [],
   "source": [
    "# Function definition\n",
    "def load_cora():\n",
    "    num_nodes = 2708\n",
    "    num_features = 1433\n",
    "    num_classes = 7\n",
    "\n",
    "    # Simulate feature matrix and labels\n",
    "    features = np.random.rand(num_nodes, num_features).astype(np.float32)\n",
    "    labels = np.random.randint(0, num_classes, size=(num_nodes,), dtype=np.int64)\n",
    "\n",
    "    # Simulate adjacency matrix (sparse graph structure)\n",
    "    row = np.random.randint(0, num_nodes, size=10000)\n",
    "    col = np.random.randint(0, num_nodes, size=10000)\n",
    "    data = np.ones(len(row), dtype=np.float32)\n",
    "    adjacency = coo_matrix((data, (row, col)), shape=(num_nodes, num_nodes))\n",
    "\n",
    "# Loop execution\n",
    "    # Define train mask (e.g., first 140 nodes for training)\n",
    "    train_mask = np.zeros(num_nodes, dtype=bool)\n",
    "    train_mask[:140] = True\n",
    "\n",
    "    return features, labels, adjacency, train_mask\n",
    "\n",
    "# Load data\n",
    "features, labels, adjacency, train_mask = load_cora()\n",
    "\n",
    "# Convert adjacency matrix to TensorFlow SparseTensor\n",
    "adj_tensor = tf.sparse.SparseTensor(\n",
    "    indices=np.vstack((adjacency.row, adjacency.col)).T,\n",
    "    values=adjacency.data,\n",
    "    dense_shape=adjacency.shape\n",
    ")\n",
    "adj_tensor = tf.sparse.reorder(adj_tensor)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = features.shape[1]\n",
    "hidden_dim = 16\n",
    "output_dim = labels.max() + 1\n",
    "\n",
    "# Initialize and train the model\n",
    "model = FastGCN(input_dim, hidden_dim, output_dim, dropout_rate=0.5)\n",
    "train(model, features, adj_tensor, labels, train_mask, num_epochs=50, num_samples=500, learning_rate=0.001)\n",
    "\n",
    "# Conditional check\n",
    "# Evaluate the model on the entire dataset (or a subset if needed)\n",
    "accuracy, _, _ = evaluate_model(\n",
    "    model = model,\n",
    "    features = features,\n",
    "    adj = adj_tensor,\n",
    "    labels = labels,\n",
    "    num_samples=features.shape[0])\n",
    "\n",
    "print(f\"Test Accuracy on CORA: {accuracy.numpy():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chS0eOUxyoQR"
   },
   "outputs": [],
   "source": [
    "# Load the CiteSeer dataset\n",
    "dataset = Planetoid(root='/tmp/CiteSeer', name='CiteSeer')\n",
    "data = dataset[0]  # Get the data object\n",
    "\n",
    "# Loop execution\n",
    "# Convert features and labels to NumPy for TensorFlow compatibility\n",
    "features = data.x.numpy().astype(np.float32)  # Feature matrix\n",
    "labels = data.y.numpy().astype(np.int64)  # Node labels\n",
    "\n",
    "# Convert adjacency matrix to SciPy sparse matrix\n",
    "adj_matrix = to_scipy_sparse_matrix(data.edge_index).tocoo()\n",
    "\n",
    "# Convert adjacency matrix to TensorFlow SparseTensor\n",
    "adj_tensor = tf.sparse.SparseTensor(\n",
    "    indices=np.vstack((adj_matrix.row, adj_matrix.col)).T,\n",
    "    values=adj_matrix.data,\n",
    "    dense_shape=adj_matrix.shape\n",
    ")\n",
    "adj_tensor = tf.sparse.reorder(adj_tensor)\n",
    "\n",
    "# Prepare train, validation, and test masks\n",
    "num_nodes = features.shape[0]\n",
    "\n",
    "train_mask = np.zeros(num_nodes, dtype=bool)\n",
    "train_mask[data.train_mask.numpy()] = True\n",
    "\n",
    "val_mask = np.zeros(num_nodes, dtype=bool)\n",
    "val_mask[data.val_mask.numpy()] = True\n",
    "\n",
    "test_mask = np.zeros(num_nodes, dtype=bool)\n",
    "test_mask[data.test_mask.numpy()] = True\n",
    "\n",
    "# Debugging information\n",
    "print(\"Dataset:\", dataset)\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Feature matrix shape:\", features.shape)\n",
    "print(\"Number of edges:\", adj_matrix.nnz)\n",
    "print(\"Number of classes:\", np.max(labels) + 1)\n",
    "print(\"Train nodes:\", np.sum(train_mask))\n",
    "print(\"Validation nodes:\", np.sum(val_mask))\n",
    "print(\"Test nodes:\", np.sum(test_mask))\n",
    "\n",
    "# Model parameters\n",
    "input_dim = features.shape[1]\n",
    "# Conditional check\n",
    "hidden_dim = 128  # Adjust hidden dimension if needed\n",
    "output_dim = labels.max() + 1\n",
    "\n",
    "# Initialize the FastGCN model\n",
    "model = FastGCN(input_dim, hidden_dim, output_dim, dropout_rate=0.5)\n",
    "\n",
    "# Training parameters\n",
    "num_samples = 500  # Number of nodes sampled per epoch\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Train the model\n",
    "train(\n",
    "    model=model,\n",
    "    features=features,\n",
    "    adj=adj_tensor,\n",
    "    labels=labels,\n",
    "    train_mask=train_mask,\n",
    "    num_epochs=num_epochs,\n",
    "    num_samples=num_samples,\n",
    "    learning_rate=learning_rate\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_accuracy, _, _ = evaluate_model(\n",
    "    model=model,\n",
    "    features=features,\n",
    "    adj=adj_tensor,\n",
    "    labels=labels,\n",
    "    num_samples=np.sum(test_mask)\n",
    ")\n",
    "print(f\"Test Accuracy on CiteSeer: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
